@article{tanAdaptiveSafeControl2024,
  title = {Adaptive Safe Control of Quadcopter: A Hierarchical Safe Reinforcement Learning Approach},
  author = {Tan, Jun Kai and Xue, Shuang Si and Guo, Zi Hang and Li, Huan and Zheng, Xiao Dong and Cao, Hui},
  date = {2024},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Eng. Appl. Artif. Intell.},
  abstract = {In this paper, a hierarchical safe reinforcement learning control framework is developed to achieve safe tracking control of the quadcopter by learning from the demonstration of human pilots. This hierarchical framework contains two levels. First, the higher level includes a motion planner which learns from the collected data of pilot maneuvers. Then, the low-level controller tracks the high-level replanned motion and avoids obstacles, which provides a safe guarantee for the quadcopter. The human pilot’s intention is inferred with the technique of dynamic movement primitives, which learns from the pilot-maneuvered trajectory and replans a similar but more efficient trajectory. Safe reinforcement learning is implemented at the low level to track the replanned trajectory and avoid obstacles. The stability of the tracking control is analyzed using the Lyapunov function. Two hardware experiments are provided to validate the effectiveness of the proposed hierarchical safe reinforcement learning framework for the safe tracking control of the quadcopter.},
  langid = {english},
  keywords = {/unread},
  annotation = {JCR分区: Q1\\
中科院分区升级版: 计算机科学2区\\
影响因子: 7.5\\
5年影响因子: 7.4\\
EI: 是\\
南农高质量: A}
}

@article{tanCompositeLearningbasedFixedtime2025,
  title = {Composite Learning-Based Fixed-Time Optimized Shared Prescribed-Performance Control for Human-Robotics Cooperative Game},
  author = {Tan, Jun Kai and Xue, Shuang Si and Guo, Zi Hang and Niu, Tian Sen and Cao, Hui and Chen, Ba Dong},
  date = {2025},
  journaltitle = {Information Sciences},
  shortjournal = {Inf. Sci.},
  abstract = {Optimizing the cooperative interaction between expert operators (experienced humans) and autonomous controllers is essential for maximizing performance in complex cyber-physical systems (CPS). While both human expertise and autonomous robotics offer unique advantages, effectively integrating their complementary capabilities remains challenging, particularly under uncertainties and input constraints. This paper investigates the composite learning-based fixedtime optimized shared prescribed-performance control (CL-FxT-OSPPC) in the context of human-robotics cooperative game (HRCG). A smooth shared mechanism within the framework of nonzero-sum game is constructed to balance the effects and trade-offs between the humanoperator and autonomous controller. Prescribed performance tracking with bounded error trajectories is achieved through performance transformation and input cost functions. Fixed-time composite learning is proposed for the approximation of optimal shared controllers within fixedtime, in which the experience relay of the human-operator and autonomous controller is blended and integrated adaptively. Lyapunov-based stability analysis is conducted to ensure fixed-time convergence of the shared controller. Extensive simulation results on dubin car and UAV control demonstrate the effectiveness of the proposed structure.},
  langid = {english},
  keywords = {/unread},
  annotation = {中科院分区升级版: 计算机科学1区\\
EI: 是\\
南农高质量: A}
}

@article{tanDatadrivenFixedtimeInverse2025,
  title = {Data-Driven {{Fixed-time Inverse Optimal Shared Control}} for {{Human-UAV Interaction}}},
  author = {Tan, Jun Kai and Xue, Shuang Si and Cao, Hui and Chen, Ba Dong},
  date = {2025},
  journaltitle = {IEEE Transactions on Artificial Intelligence},
  shortjournal = {IEEE Trans. Artif. Intell.},
  langid = {english},
  keywords = {/unread},
  annotation = {EI: 是}
}

@article{tanDatadrivenOptimalShared2025,
  title = {Data-Driven Optimal Shared Control of Unmanned Aerial Vehicles},
  author = {Tan, Junkai and Xue, Shuangsi and Guo, Zihang and Li, Huan and Cao, Hui and Chen, Badong},
  date = {2025-03},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {622},
  pages = {129428},
  issn = {09252312},
  doi = {10.1016/j.neucom.2025.129428},
  abstract = {Cooperation between humans and autonomy is a critical topic of unmanned aerial vehicle (UAV) control. How to co-pilot the UAV with human operator to achieve optimal performance presents a significant challenge. In this paper, we propose a novel data-driven optimal shared control method for UAV using the Koopman operators to predict the nonlinear dynamics of the UAVs. An original shared control mechanism is established to allocate the relationship between optimal and human control inputs. The model of the system is learned from human maneuver data via the Koopman operator approach, and the optimal controller is approximated online using reinforcement learning techniques. The Lyapunov theory analyzes the stability of the proposed method. Compared with offline RL methods, the proposed method can learn the optimal controller online without a precise UAV dynamics model from human maneuver data. The effectiveness of the proposed method is demonstrated by numerical and Human-in-the-loop (HiTL) simulation.},
  langid = {english},
  keywords = {/unread},
  annotation = {JCR分区: Q1\\
中科院分区升级版: 计算机科学2区\\
影响因子: 5.5\\
5年影响因子: 5.5\\
EI: 是}
}

@article{tanFinitetimeSafeReinforcement2025,
  title = {Finite-Time Safe Reinforcement Learning Control of Multi-Player Nonzero-Sum Game for Quadcopter Systems},
  author = {Tan, Junkai and Xue, Shuangsi and Guan, Qingshu and Qu, Kai and Cao, Hui},
  date = {2025-09},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {712},
  pages = {122117},
  issn = {00200255},
  doi = {10.1016/j.ins.2025.122117},
  langid = {english},
  annotation = {https://github.com/tanjunkai2001/FT-SRL-Quadcopter\\
中科院分区升级版: 计算机科学2区\\
EI: 是}
}

@article{tanFiniteTimeStackelbergGameBased2025,
  title = {Finite-{{Time Stackelberg Game-Based Hybrid Attack-Defense Control}} for {{Cyber-Physical Systems}}},
  author = {Tan, Jun Kai and Xue, Shuang Si and Cao, Hui and Chen, Ba Dong},
  date = {2025},
  journaltitle = {IEEE/CAA Journal of Automatica Sinica},
  shortjournal = {IEEE/CAA J. Autom. Sinica},
  langid = {english},
  keywords = {/unread},
  annotation = {JCR分区: Q1\\
中科院分区升级版: 计算机科学1区\\
影响因子: 15.3\\
5年影响因子: 10.3\\
EI: 是}
}

@article{tanFixedtimeConcurrentLearningbased2025,
  title = {Fixed-Time Concurrent Learning-Based Robust Approximate Optimal Control},
  author = {Tan, Junkai and Xue, Shuangsi and Niu, Tiansen and Qu, Kai and Cao, Hui and Chen, Badong},
  date = {2025-05-06},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn},
  issn = {1573-269X},
  doi = {10.1007/s11071-025-11235-8},
  abstract = {In this paper, we investigate a fixed-time concurrent learning-based actor-critic-identifier (FxT-CL-ACI) control scheme for approximating the optimal tracking controller and identifying uncertain system parameters online. The proposed FxT-CL-ACI control scheme is applied to solve the robust optimal tracking control problem for uncertain nonlinear systems with disturbances and actuator saturation. The interaction between the leader and follower in the Stackelberg game is modeled to achieve robust optimal tracking control with sequential optimization of \$\$H\_2\$\$and \$\$H\_\{\textbackslash infty \}\$\$performance indices. The effectiveness of the proposed FxT-CL-ACI control scheme is demonstrated by a numerical simulation and a hardware experiment on a UAV system. The results show that the FxT-CL-ACI control scheme can achieve robust optimal tracking control with fixed-time convergence and disturbance rejection, even in the presence of actuator saturation and uncertain system parameters.},
  langid = {english},
  keywords = {/unread,Actor-critic-identifier,Approximate optimal control,Continuous Optimization,Control and Systems Theory,Fixed-time concurent learning,Learning Theory,Neural network,Stackelberg game,Stochastic Learning and Adaptive Control,Stochastic Systems and Control,Systems Theory Control}
}

@article{tanFixedTimeHierarchicalGameBased2025,
  title = {Fixed-{{Time Hierarchical Game-Based Unmanned Aerial-Ground Vehicle Docking Control}}},
  author = {Tan, Jun Kai and Xue, Shuang Si and Guo, Zi Hang and Cao, Hui and Chen, Ba Dong},
  date = {2025},
  journaltitle = {IEEE/CAA Journal of Automatica Sinica},
  shortjournal = {IEEE/CAA J. Autom. Sinica},
  langid = {english},
  keywords = {/unread},
  annotation = {titleTranslation: Fixed-Time Hierarchical Game-Based Unmanned Aerial-Ground Vehicle Docking Control\\
JCR分区: Q1\\
中科院分区升级版: 计算机科学1区\\
影响因子: 15.3\\
5年影响因子: 10.3\\
EI: 是}
}

@article{tanHierarchicalSafeReinforcement2025,
  title = {Hierarchical Safe Reinforcement Learning Control for Leader-Follower Systems with Prescribed Performance},
  author = {Tan, Junkai and Xue, Shuangsi and Li, Huan and Guo, Zihang and Cao, Hui and Chen, Badong},
  date = {2025},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  shortjournal = {IEEE Trans. Autom. Sci. Eng.},
  pages = {1--1},
  issn = {1558-3783},
  doi = {10.1109/TASE.2025.3596912},
  abstract = {This paper proposes a hierarchical safe reinforcement learning with prescribed performance control (HSRL-PPC) scheme to address the challenges of interconnected leader-follower systems operating in complex environments. The framework consists of two levels: at the higher level, the leader agent detects and avoids moving obstacles while planning optimal paths; at the lower level, the follower agent tracks the leader within strict prescribed performance bounds. We formulate the optimal prescribed performance safe control problem and solve it using the Hamilton-Jacobi-Bellman (HJB) equation. Due to system nonlinearity and obstacle complexity, we approximate the leader’s optimal value function using a state-following neural network that efficiently extrapolates training data to neighboring states, while employing a regular critic neural network for the follower’s value function approximation. Lyapunov stability analysis demonstrates the closed-loop system’s theoretical guarantees. Experimental results from two simulation examples and hardware tests with a quadcopter-vehicle system validate the effectiveness of the proposed approach in achieving safe navigation and precise tracking performance in dynamic environments.},
  langid = {english},
  keywords = {/unread}
}

@article{tanHumanAIInteractive2025,
  title = {Human–{{AI}} Interactive Optimized Shared Control},
  author = {Tan, Junkai and Xue, Shuangsi and Cao, Hui and Ge, Shuzhi Sam},
  date = {2025-01},
  journaltitle = {Journal of Automation and Intelligence},
  shortjournal = {Journal of Automation and Intelligence},
  pages = {S2949855425000024},
  issn = {29498554},
  doi = {10.1016/j.jai.2025.01.001},
  abstract = {This paper presents an optimized shared control algorithm for human–AI interaction, implemented through a digital twin framework where the physical system and human operator act as the real agent while an AIdriven digital system functions as the virtual agent. In this digital twin architecture, the real agent acquires an optimal control strategy through observed actions, while the AI virtual agent mirrors the real agent to establish a digital replica system and corresponding control policy. Both the real and virtual optimal controllers are approximated using reinforcement learning (RL) techniques. Specifically, critic neural networks (NNs) are employed to learn the virtual and real optimal value functions, while actor NNs are trained to derive their respective optimal controllers. A novel shared mechanism is introduced to integrate both virtual and real value functions into a unified learning framework, yielding an optimal shared controller. This controller adaptively adjusts the confidence ratio between virtual and real agents, enhancing the system’s efficiency and flexibility in handling complex control tasks. The stability of the closed-loop system is rigorously analyzed using the Lyapunov method. The effectiveness of the proposed AI–human interactive system is validated through two numerical examples: a representative nonlinear system and an unmanned aerial vehicle (UAV) control system.},
  langid = {english},
  keywords = {/unread},
  annotation = {GSCC: 0000005 2025-03-30T02:17:56.331Z \\
0 citations (Semantic Scholar/DOI) [2025-03-11]\\
0 citations (Semantic Scholar/DOI) [2025-03-11]\\
0 citations (Crossref/DOI) [2025-03-11]}
}

@article{tanHumanMachineShared2025,
  title = {Human–{{Machine Shared Stabilization Control Based}} on {{Safe Adaptive Dynamic Programming With Bounded Rationality}}},
  author = {Tan, Junkai and Wang, Jingcheng and Xue, Shuangsi and Cao, Hui and Li, Huan and Guo, Zihang},
  date = {2025},
  journaltitle = {International Journal of Robust and Nonlinear Control},
  volume = {35},
  number = {11},
  pages = {4638--4657},
  issn = {1099-1239},
  doi = {10.1002/rnc.7931},
  abstract = {This article considers the shared control of bounded rational human behavior with cooperative autonomous machines. For the collaboration of humans and machines, it is crucial to ensure the safety of the interactive process due to the involvement of human beings. First, a barrier-function-based state transformation is developed to ensure full state safety constraints. A level-\textbackslash{} k \textbackslash{} thinking framework is exploited to obtain bounded rationality. Every single level-\textbackslash{} k \textbackslash{} control policy is approximated by using adaptive dynamic programming. Inspired by the theory of human behavior modeling, a probabilistic distribution based on Softmax is utilized to model human behavior, which imitates the uncertainty of human intelligence in the cooperative game. Through the construction of a shared control framework, the control inputs of humans and machines are blended to achieve stabilization safely and efficiently. Finally, simulations are implemented to test the effectiveness of the proposed cooperation architecture. The result demonstrates that full-state asymmetric constraints and stabilization are guaranteed in commonly safety-critical situations, and the shared control framework ensures the safety of the overall system when one of the participants is not safety-aware.},
  langid = {english},
  keywords = {/unread,adaptive dynamic programming,barrier function,bounded rationality,human–machine collaboration,shared control}
}

@thesis{TanJiYuZiGuaYingDongTaiGuiHuaDeHuLianXiTongAnQuanBaoZhangKongZhi2023,
  type = {本科毕业设计},
  title = {基于自适应动态规划的互联系统安全保障控制},
  author = {谭, 浚楷},
  date = {2023-06},
  institution = {西安交通大学},
  location = {陕西，西安},
  url = {../assets/基于自适应动态规划的互联系统安全保障控制.pdf},
  abstract = {随着工业系统规模的增大以及人工智能技术特别是机器学习的发展，大规模复杂系统的控制方法在多个领域得到应用。作为复杂系统中的基础问题，互联系统稳定控制以及智能体博弈纳什均衡往往具有很强的耦合性以及非线性，也一直是多智能体领域的研究热点。大规模系统通常是重点安全控制系统，需要通过一定的优化约束条件来满足系统的安全运行要求。为了保障系统运行的安全稳定，可以在系统的控制层面加入安全保障机制。安全保障控制器就是一种在原有的系统层级控制策略架构上拓展的决策机制，通过对系统危险倾向进行反向补偿，实现整体状态的安全稳定运行，因此安全保障控制器的研究对重点安全的大规模系统稳定安全具有很高的研究价值。 本文主要设计了一种针对包括互联系统、非零和博弈在内的复杂系统的安全保障控制器，不仅能够满足凸集合的安全边界约束要求，还可以满足传统安全控制器无法满足的非凸集合安全约束要求。同时安全保障控制补偿项可以和任意名义控制器结合，灵活适用各种系统，对系统参数的变化具有良好的抗干扰性，对安全边界的约束具有一定的自适应性。本文分别针对复杂系统中的非零和博弈系统以及互联系统的特性进行安全保障控制器设计。这种保障安全的控制器设计主要分为三个部分。首先设计一种基于障碍函数梯度的安全保障的控制器补偿项。其次根据自适应动态规划理论建立名义控制器，非零和博弈系统中需要满足多个智能体非零和博弈下的纳什均衡，而互联系统中则需要利用分布式稳定性理论设计分布式稳定控制器。最后利用并行学习的更新律对控制器逼近的神经网络进行迭代，借助单一评判网络方法，在得到适用性广泛的安全保障控制器的同时，减少了神经网络逼近参数迭代过程中一半的计算量。 在此基础上，本文借助李雅普诺夫稳定性理论分别证明了非零和博弈与互联系统在安全保障控制器作用下的稳定性，并且证明得到并行学习更新律的参数最终一致有界。本文通过数值仿真实验对互联系统以及非零和博弈的理论分析结果进行验证，表明在所设计的基于障碍函数的安全保障控制器的作用下，复杂系统能够在非凸性以及凸性安全约束下保障状态变量的安全。 为了进一步验证基于障碍函数的安全保障策略在实际应用场景下的效果，本文设计了无人机的硬件模拟实验对安全保障策略进行验证，表明在所设计的安全策略作用下，无人机复杂系统能够在避开固定障碍物以及无人机互相避让控制下保障安全。},
  langid = {cn},
  pagetotal = {51},
  keywords = {/unread,⭐⭐},
  annotation = {https://github.com/tanjunkai2001/human-machine-safe}
}

@article{TanJunKaiRongHeWuMoXingQiangHuaXueXiDeYongCiTongBuDianJiHunDunKangRaoKongZhi2025,
  title = {融合无模型强化学习的永磁同步电机混沌抗扰控制},
  author = {{谭浚楷} and {薛霜思} and {郭子航} and {曹晖}},
  date = {2025-06-17},
  journaltitle = {智能科学与技术学报},
  shortjournal = {智能科学与技术学报},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=ZNJS2025061300B},
  urldate = {2025-08-09},
  abstract = {针对永磁同步电机（permanent magnet synchronous motor,PMSM）控制中出现的由非线性震荡引发的混沌行为，提出了一种利用电机历史运行数据迭代获取最优控制策略的无模型强化学习方法，该方法解决了不同工作环境下因负载扰动不确定、电机模型参数不确定带来的问题。首先针对外部负载扭矩的不确定性，建立电机最优控制与外部最坏扰动之间的零和博弈，设计迭代形式的黎卡提方程，基于该方程进一步构建了一种基于模型的抗干扰最优控制器。在该控制器基础上，引入无模型强化学习思想，设计了PMSM混沌现象的数据驱动稳定方法，利用电机历史运行数据实现优化求解无模型抗扰动稳定控制器，实现对电机混沌行为的抗扰稳定控制。最后通过多个数值仿真实验对比验证了所设计方法的性能。实验结果显示，在外界负载扰动不确定的情况下，提出的方法相较传统有限时间控制方法在累计资源消耗损失性能方面提升了39.04\%，在电机模型参数不确定的情况下，提出的方法相比线性二次型调节器的控制成功率提高10.71\%。},
  langid = {chinese},
  pubstate = {advance online publication},
  keywords = {/unread},
  annotation = {foundation: 中国博士后科学基金（No.2024M762602）；\\
download: 105\\
CLC: TM341;TP273\\
dbcode: CAPJ\\
dbname: CAPJLAST\\
filename: ZNJS2025061300B}
}

@inproceedings{tanNashEquilibriumSolution2023,
  title = {Nash {{Equilibrium Solution Based}} on {{Safety-Guarding Reinforcement Learning}} in {{Nonzero-Sum Game}}},
  booktitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Tan, Junkai and Xue, Shuangsi and Cao, Hui and Li, Huan},
  date = {2023-07-08},
  pages = {630--635},
  doi = {10.1109/ICARM58088.2023.10218910},
  abstract = {In this paper, a safety-guarding controller is introduced to keep the safety of exploration in constrained state space. The controller is utilized to obtain the nonzero-sum game Nash equilibrium solution via a model-based reinforcement learning architecture. To deal with the uncertainty of persistent excitation, a concurrent learning approach is applied and both historical and transient data are employed in the learning process. In order to reduce the computational load, a single-critic network is utilized for approximation. To demonstrate the effectiveness of the proposed method, a two-player nonzero-sum game is developed, toward both convex/non-convex safe state-space constraints.},
  eventtitle = {2023 {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  langid = {english},
  keywords = {/unread},
  annotation = {1 citations (Semantic Scholar/DOI) [2025-03-11]\\
0 citations (Crossref/DOI) [2025-03-11]\\
GSCC: 0000003 2025-03-30T03:51:06.271Z \\
TLDR: A safety-guarding controller is introduced to keep the safety of exploration in constrained state space and the nonzero-sum game Nash equilibrium solution via a model-based reinforcement learning architecture is obtained.}
}

@article{tanPrescribedPerformanceRobust2025,
  title = {Prescribed {{Performance Robust Approximate Optimal Tracking Control}} via {{Stackelberg Game}}},
  author = {Tan, Junkai and Xue, Shuangsi and Li, Huan and Guo, Zihang and Cao, Hui and Li, Dongyu},
  date = {2025},
  journaltitle = {IEEE Transactions on Automation Science and Engineering},
  volume = {22},
  pages = {12871--12883},
  issn = {1558-3783},
  doi = {10.1109/TASE.2025.3549114},
  abstract = {Real-world applications of nonlinear systems tracking control are always challenging due to the existence of uncertainties and disturbances. To design a robust optimal tracking controller for uncertain nonlinear systems with disturbances and actuator saturation, this paper investigates the prescribed performance robust optimal tracking control problem. A prescribed performance mechanism is constructed to convert the dynamics of tracking error into transformed error dynamics, which keeps the system’s operating states within specific bounds, ensuring tracking with predefined error constraints. For the optimal tracking controller design, an optimal index is established to optimize the performance of tracking control, and a robust optimal index is established to optimize the disturbance effect on the tracking error. To achieve robust optimal tracking control that minimizes both optimal and robust optimal indexes, a Stackelberg game is constructed, which provides a hierarchical game structure for the optimal controller and the worst disturbance. The robust optimal controller is approximated online using reinforcement learning techniques. An actor-critic-identifier algorithm is designed to approximate the optimal value function, optimal controller, and drifted system parameters. Lyapunov theory is utilized to analyze the closed-loop system’s stability. To demonstrate the effectiveness of the proposed robust optimal control method, two numerical simulations and a hardware experiment on a quadcopter system are conducted. The experiment results demonstrate that our method successfully achieves prescribed performance tracking control when actuators are saturated and disturbances are present. Note to Practitioners—In this paper, the probelm of mixed H\_2/H\_ınfty prescribed-performance optimal tracking control for nonlinear systems with input saturation is investigated. To constrain the operating states of the system within certain bounds, the prescribed performance transformation is designed to achieve tracking with predefined error constraints. For the optimal controller design, the H₂ index is established to minimize the optimal tracking performance, and the H\textsubscript{ınfty } index is designed to minimize the disturbance effect on the tracking error. A Stackelberg-based non-zero sum game between the optimal controller and the worst disturbance is established to design the mixed H\_2/H\_ınfty optimal tracking controller. The designed optimal controller is approximated online using reinforcement learning. Effectiveness of the proposed method is demonstrated by two numerical simulations and a hardware experiment on a quadcopter system. Based on the proposed high-performance controller, engineers can design a high-performance robust optimal tracking controller for uncertain nonlinear systems with extreme conditions of disturbances and actuator saturation.},
  keywords = {/unread,actor-critic,Actuators,approximate optimal control,Approximation algorithms,Degradation,Games,Indexes,Insulation life,Nonlinear dynamical systems,Prescribed performance control,Robustness,Stackelberg game,system identification,Trajectory,Uncertainty}
}

@inproceedings{tanSafeHumanMachineCooperative2023,
  title = {Safe {{Human-Machine Cooperative Game}} with {{Level-k Rationality Modeled Human Impact}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  author = {Tan, Junkai and Xue, Shuangsi and Cao, Hui and Li, Huan},
  date = {2023-11-09},
  pages = {188--193},
  doi = {10.1109/ICDL55364.2023.10364413},
  abstract = {This paper considers the problem of bounded rational human behavior in the cooperative human-machine game. The cooperation between human and machine is a raising topic for emergency handling, and it is critical to ensure the safety of human. First, a barrier-function-based state transformation is developed to ensure the safety constraints of the human-machine system state. A level-k rationality structure is then exploited by cognitive hierarchy to learn human behavior, and the bounded rational behavior is obtained by using Adaptive Dynamic Programming (ADP). Inspired by behavior modeling from sociology, a softmax probabilistic decision distribution is utilized to model human behavior, which imitates the true impact of human in the cooperative game. Finally, a simulation is implemented to test the effectiveness of the proposed behavior, which demonstrates that the full state constraints and stabilization are guaranteed.},
  eventtitle = {2023 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  langid = {english},
  keywords = {/unread},
  annotation = {GSCC: 0000003 2025-03-30T03:51:02.278Z \\
1 citations (Semantic Scholar/DOI) [2025-03-11]\\
1 citations (Semantic Scholar/DOI) [2025-03-11]\\
0 citations (Crossref/DOI) [2025-03-11]}
}

@inproceedings{tanSafeStabilizationControl2024,
  title = {Safe Stabilization Control for Interconnected Virtual-Real Systems via Model-Based Reinforcement Learning},
  booktitle = {2024 14th {{Asian Control Conference}} (Ascc)},
  author = {Tan, Junkai and Xue, Shuangsi and Li, Huan and Cao, Hui and Li, Dongyu},
  date = {2024-07},
  pages = {605--610},
  url = {https://ieeexplore.ieee.org/document/10665364/?arnumber=10665364},
  urldate = {2024-09-29},
  abstract = {In this paper, a safe-guarding controller is designed for the interconnected virtual-real system based on a reinforcement learning framework to achieve stabilization control. We established the mathematical formulation of the interconnected virtual-real system and the safety-guaranteed stabilization optimization problem. Online reinforcement learning methods are utilized to solve the Hamilton-Jacobi-Bellman(HJB) equation on the established optimal control problem. The safe-guarding term is introduced to achieve safe-guarding control for the real part. Single network is used to approximate the value function. Concurrent Learning methods are introduced to train the network without excitation risks. We prove that the dynamics of the estimation error of the designed critic network are uniform and ultimately bounded. Finally, a numerical simulation example is provided to illustrate the effectiveness of the proposed control method.},
  eventtitle = {2024 14th {{Asian Control Conference}} ({{ASCC}})},
  langid = {english},
  keywords = {Estimation error,Interconnected virtual-real system,Learning systems,Mathematical models,notion,Numerical models,Numerical simulation,Optimal control,reinforcement learning,Reinforcement learning,safety-guaranteed,stabilization control},
  annotation = {EI: 是}
}

@article{tanStackelbergGamebasedRobust2025,
  title = {Stackelberg Game-Based Robust Optimal Control of Cyber-Physical System under Hybrid Attacks},
  author = {Tan, Junkai and Xue, Shuangsi and Cao, Hui},
  date = {2025-02-27},
  journaltitle = {The International Journal of Intelligent Control and Systems},
  shortjournal = {Int. J. Intell. Control Syst.},
  pages = {9},
  url = {https://www.ijics.cn/article/145},
  abstract = {This paper presents a novel framework integrating Stackelberg game theory and reinforcement learning for cyberphysical system (CPS) security. We develop a hierarchical game model where defenders and attackers interact through sequential decision-making. The defender-attacker dynamics are formulated as an optimization problem combining H2 and H∞ control objectives. Key innovations include: 1) A unified game-theoretic approach for modeling hybrid attack-defense mechanisms, 2) Online reinforcement learning algorithms for real-time strategy adaptation, and 3) Rigorous stability analysis using Lyapunov theory. Theoretical guarantees of convergence are established for the proposed learning scheme. Comprehensive experiments on a robotic platform validate the framework’s effectiveness in maintaining control performance under diverse attack scenarios.},
  langid = {english}
}

@article{tanUnmannedAerialgroundVehicle2025,
  title = {Unmanned Aerial-Ground Vehicle Finite-Time Docking Control via Pursuit-Evasion Games},
  author = {Tan, Junkai and Xue, Shuangsi and Guan, Qingshu and Niu, Tiansen and Cao, Hui and Chen, Badong},
  date = {2025-07-01},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn},
  volume = {113},
  number = {13},
  pages = {16757--16777},
  issn = {1573-269X},
  doi = {10.1007/s11071-025-11021-6},
  abstract = {Cooperation between unmanned autonomous systems has attracted increasing attention in recent years, particularly the challenging problem of unmanned aerial vehicle (UAV) and unmanned ground vehicle (UGV) docking in complex environments with dynamic vehicle interactions. This paper proposes a novel finite-time reinforcement learning control scheme for UAV–UGV docking based on a pursuit-evasion game framework. A pursuit-evasion game formulation is developed where the evader vehicle navigates through complex environments while being pursued by a pursuer vehicle required to track and dock with it. The docking performance is optimized through achieving Nash equilibrium of the pursuit-evasion game. The proposed finite-time reinforcement learning algorithm transforms the value function to finite-time space and employs Actor-Critic neural networks to approximate the value function and optimal controller. A finite-time concurrent learning law is utilized to update the neural network weights, ensuring both the pursuit-evasion game equilibrium and learning process converge within finite time. Lyapunov stability analysis proves the finite-time convergence properties of the algorithm. Experimental validation on an aerial-ground vehicle system demonstrates the effectiveness of the proposed approach in achieving optimal pursuit-evasion performance while maintaining safe landing capability.},
  langid = {english},
  keywords = {/unread,Control and Systems Theory,Docking control,Finite-time reinforcement learning,Game Theory,Pursuit-evasion game,Smooth pursuit,Spike-timing-dependent plasticity,Stochastic Learning and Adaptive Control,Stochastic Systems and Control,Unmanned aerial vehicle,Unmanned ground vehicle}
}

@article{xueCooperativeGamebasedOptimal2025,
  title = {Cooperative Game-Based Optimal Shared Control of Unmanned Aerial Vehicle},
  author = {Xue, Shuangsi and Tan, Junkai and Guo, Zihang and Guan, Qingshu and Qu, Kai and Cao, Hui},
  date = {2025-04},
  journaltitle = {Unmanned Systems},
  shortjournal = {Unmanned Syst.},
  doi = {doi.org/10.1142/S2301385026500342},
  langid = {english},
  keywords = {/unread},
  annotation = {中科院分区升级版: 计算机科学3区\\
影响因子: 3.0\\
5年影响因子: 3.3\\
EI: 是}
}

@article{xueFinitetimeDynamicEventtriggered2024,
  title = {Finite-Time Dynamic Event-Triggered Actor-Critic-Identifier for Optimal Control of Nonlinear Drifted System},
  author = {Xue, Shuang Si and Tan, Jun Kai and Guo, Zi Hang and Guan, Qing Shu and Cao, Hui},
  date = {2024},
  journaltitle = {Nonlinear Dynamics},
  shortjournal = {Nonlinear Dyn.},
  abstract = {In this paper, a finite-time dynamic eventtriggered actor-critic-identifier (FT-DET-ACI) algorithm is studied for the optimal control of nonlinear systems with unknown drifted dynamics. To formulate the problem of finite-time optimal control, the value function in the finite-time stable space is defined to facilitate the finite-time stabilization of the system states. The finite-time optimal controller is obtained through the solution of the transformed Hamilton-Jacobi-Bellman (HJB) equation. For solving the HJB equation with unknown drifted dynamics, the actor-critic-identifier (ACI) structure is proposed to approximate the value function and finite-time optimal controller with drifted system dynamic parameters. A dynamic event-trigger (DET) rule is employed to alleviate the computational burden of the fractional sign function and to facilitate communication between the controller and the plant. Lyapunov stability analysis is utilized to analyze the finitetime stability of the closed-loop system. Numerical simulations are presented to demonstrate the effectiveness of the proposed FT-DET-ACI algorithm.},
  langid = {english},
  keywords = {/unread},
  annotation = {JCR分区: Q1\\
中科院分区升级版: 工程技术2区\\
影响因子: 5.2\\
5年影响因子: 4.8\\
EI: 是\\
南农高质量: A}
}

@article{xuePrescribedPerformanceOptimized2025,
  title = {Prescribed Performance Optimized Control of {{UAV}} with Robust Approximate Dynamic Programming under Disturbance},
  author = {Xue, Shuang Si and Tan, Jun Kai and Niu, Tian Sen and Qu, Kai and Cao, Hui and Chen, Ba Dong},
  date = {2025},
  journaltitle = {IEEE Transactions on Industrial Electronics},
  shortjournal = {IEEE Trans. Ind. Electron.},
  abstract = {This study proposes an integrated control framework that combines prescribed performance backstepping with reinforcement learning (RL) optimization for quadrotor unmanned aerial vehicles (QUAVs) operating under disturbances. The key innovation lies in enforcing guaranteed transient and steady-state tracking performance while achieving optimal control behavior through online learning. A systematic two-stage control architecture is developed: An outer-loop position controller incorporating prescribed performance bounds handles the underactuated dynamics, complemented by an inner-loop attitude controller ensuring desired orientation tracking. The inner-loop controller employ critic-actor neural networks to approximate optimal solutions of the Hamilton-Jacobi-Bellman (HJB) equation, while the outer-loop controller optimizes position tracking performance through Hamilton-JacobiIsaacs (HJI) equation solutions. Rigorous stability analysis establishes asymptotic convergence using Lyapunov theory under the prescribed performance constraints. The efficacy of the proposed framework is validated through comprehensive numerical simulations, demonstrating exceptional tracking precision with position errors under prescribed performance.},
  langid = {english},
  keywords = {/unread},
  annotation = {JCR分区: Q1\\
中科院分区升级版: 工程技术1区\\
影响因子: 7.5\\
5年影响因子: 8.0\\
EI: 是\\
南农高质量: A}
}
