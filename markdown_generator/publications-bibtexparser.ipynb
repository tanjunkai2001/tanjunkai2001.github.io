{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publications markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of publications with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). The core python code is also in `publications.py`. Run either from the `markdown_generator` folder after replacing `publications.tsv` with one containing your data.\n",
    "\n",
    "TODO: Make this work with BibTex and other databases of citations, rather than Stuart's non-standard TSV format and citation style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top. \n",
    "\n",
    "- `excerpt` and `paper_url` can be blank, but the others must have values. \n",
    "- `pub_date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]`\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pandas\n",
    "\n",
    "We are using the very handy pandas library for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'PDF:/Users/tanjunkai/Zotero/storage/TWI9LZWW/Tan 等 - 2025 - Prescribed Performance Robust Approximate Optimal Tracking Control Via Stackelberg Game.pdf:application/pdf',\n",
       "  'keywords': '/unread',\n",
       "  'note': '{GSCC}: 0000000 2025-03-25T02:45:39.915Z \\n{JCR分区}: Q1\\n中科院分区升级版: 计算机科学2区\\n影响因子: 5.9\\n5年影响因子: 6.0\\n{EI}: 是',\n",
       "  'langid': 'english',\n",
       "  'date': '2025-03-07',\n",
       "  'urldate': '2025-03-18',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Li, Huan and Guo, Zihang and Cao, Hui and Li, Dongyu',\n",
       "  'shortjournal': '{IEEE} Trans. Automat. Sci. Eng.',\n",
       "  'journaltitle': '{IEEE} Transactions on Automation Science and Engineering',\n",
       "  'pages': '1--1',\n",
       "  'abstract': 'Real-world applications of nonlinear systems tracking control are always challenging due to the existence of uncertainties and disturbances. To design a robust optimal tracking controller for uncertain nonlinear systems with disturbances and actuator saturation, this paper investigates the prescribed performance robust optimal tracking control problem. A prescribed performance mechanism is constructed to convert the dynamics of tracking error into transformed error dynamics, which keeps the system’s operating states within specific bounds, ensuring tracking with predefined error constraints. For the optimal tracking controller design, an optimal index is established to optimize the performance of tracking control, and a robust optimal index is established to optimize the disturbance effect on the tracking error. To achieve robust optimal tracking control that minimizes both optimal and robust optimal indexes, a Stackelberg game is constructed, which provides a hierarchical game structure for the optimal controller and the worst disturbance. The robust optimal controller is approximated online using reinforcement learning techniques. An actor-critic-identifier algorithm is designed to approximate the optimal value function, optimal controller, and drifted system parameters. Lyapunov theory is utilized to analyze the closed-loop system’s stability. To demonstrate the effectiveness of the proposed robust optimal control method, two numerical simulations and a hardware experiment on a quadcopter system are conducted. The experiment results demonstrate that our method successfully achieves prescribed performance tracking control when actuators are saturated and disturbances are present.',\n",
       "  'doi': '10.1109/TASE.2025.3549114',\n",
       "  'url': 'https://ieeexplore.ieee.org/document/10916718/',\n",
       "  'issn': '1545-5955, 1558-3783',\n",
       "  'rights': 'https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html',\n",
       "  'title': 'Prescribed Performance Robust Approximate Optimal Tracking Control Via Stackelberg Game',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanPrescribedPerformanceRobust2025'},\n",
       " {'file': 'PDF:/Users/tanjunkai/Zotero/storage/K5AJW37B/Tan 等 - 2025 - Unmanned aerial-ground vehicle finite-time docking control via pursuit-evasion games.pdf:application/pdf',\n",
       "  'keywords': '/unread',\n",
       "  'note': 'https://github.com/tanjunkai2001/{FT}-{RL}-{PEG}-sim1\\n{JCR分区}: Q1\\n中科院分区升级版: 工程技术2区\\n影响因子: 5.2\\n5年影响因子: 4.8\\n{EI}: 是',\n",
       "  'langid': 'english',\n",
       "  'date': '2025-03-15',\n",
       "  'urldate': '2025-03-18',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Guan, Qingshu and Niu, Tiansen and Cao, Hui and Chen, Badong',\n",
       "  'shortjournal': 'Nonlinear Dyn',\n",
       "  'journaltitle': 'Nonlinear Dynamics',\n",
       "  'abstract': 'Cooperation between unmanned autonomous systems has attracted increasing attention in recent years, particularly the challenging problem of unmanned aerial vehicle ({UAV}) and unmanned ground vehicle ({UGV}) docking in complex environments with dynamic vehicle interactions. This paper proposes a novel finite-time reinforcement learning control scheme for {UAV}–{UGV} docking based on a pursuit-evasion game framework. A pursuit-evasion game formulation is developed where the evader vehicle navigates through complex environments while being pursued by a pursuer vehicle required to track and dock with it. The docking performance is optimized through achieving Nash equilibrium of the pursuit-evasion game. The proposed finite-time reinforcement learning algorithm transforms the value function to finite-time space and employs Actor-Critic neural networks to approximate the value function and optimal controller. A finite-time concurrent learning law is utilized to update the neural network weights, ensuring both the pursuit-evasion game equilibrium and learning process converge within finite time. Lyapunov stability analysis proves the finite-time convergence properties of the algorithm. Experimental validation on an aerial-ground vehicle system demonstrates the effectiveness of the proposed approach in achieving optimal pursuit-evasion performance while maintaining safe landing capability.',\n",
       "  'doi': '10.1007/s11071-025-11021-6',\n",
       "  'url': 'https://link.springer.com/10.1007/s11071-025-11021-6',\n",
       "  'issn': '0924-090X, 1573-269X',\n",
       "  'title': 'Unmanned aerial-ground vehicle finite-time docking control via pursuit-evasion games',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanUnmannedAerialgroundVehicle2025'},\n",
       " {'file': 'IEEE Xplore Full Text PDF:/Users/tanjunkai/Zotero/storage/F72MF488/Tan 等 - 2023 - Nash Equilibrium Solution Based on Safety-Guarding Reinforcement Learning in Nonzero-Sum Game.pdf:application/pdf',\n",
       "  'keywords': '/unread',\n",
       "  'note': '{GSCC}: 0000003 2025-03-20T07:54:19.911Z \\n1 citations (Semantic Scholar/{DOI}) [2025-03-11]\\n0 citations (Crossref/{DOI}) [2025-03-11]',\n",
       "  'langid': 'english',\n",
       "  'date': '2023-07-08',\n",
       "  'urldate': '2024-04-22',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Cao, Hui and Li, Huan',\n",
       "  'booktitle': '2023 International Conference on Advanced Robotics and Mechatronics ({ICARM})',\n",
       "  'pages': '630--635',\n",
       "  'eventtitle': '2023 International Conference on Advanced Robotics and Mechatronics ({ICARM})',\n",
       "  'abstract': 'In this paper, a safety-guarding controller is introduced to keep the safety of exploration in constrained state space. The controller is utilized to obtain the nonzero-sum game Nash equilibrium solution via a model-based reinforcement learning architecture. To deal with the uncertainty of persistent excitation, a concurrent learning approach is applied and both historical and transient data are employed in the learning process. In order to reduce the computational load, a single-critic network is utilized for approximation. To demonstrate the effectiveness of the proposed method, a two-player nonzero-sum game is developed, toward both convex/non-convex safe state-space constraints.',\n",
       "  'doi': '10.1109/ICARM58088.2023.10218910',\n",
       "  'url': 'https://ieeexplore.ieee.org/document/10218910/',\n",
       "  'title': 'Nash Equilibrium Solution Based on Safety-Guarding Reinforcement Learning in Nonzero-Sum Game',\n",
       "  'ENTRYTYPE': 'inproceedings',\n",
       "  'ID': 'tanNashEquilibriumSolution2023'},\n",
       " {'file': 'IEEE Xplore Full Text PDF:/Users/tanjunkai/Zotero/storage/YQFBF68Y/Tan 等 - 2023 - Safe Human-Machine Cooperative Game with Level-k Rationality Modeled Human Impact.pdf:application/pdf',\n",
       "  'keywords': '/unread',\n",
       "  'note': '{GSCC}: 0000003 2025-03-20T07:54:16.762Z \\n1 citations (Semantic Scholar/{DOI}) [2025-03-11]\\n1 citations (Semantic Scholar/{DOI}) [2025-03-11]\\n0 citations (Crossref/{DOI}) [2025-03-11]',\n",
       "  'langid': 'english',\n",
       "  'date': '2023-11-09',\n",
       "  'urldate': '2024-04-22',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Cao, Hui and Li, Huan',\n",
       "  'booktitle': '2023 {IEEE} International Conference on Development and Learning ({ICDL})',\n",
       "  'pages': '188--193',\n",
       "  'eventtitle': '2023 {IEEE} International Conference on Development and Learning ({ICDL})',\n",
       "  'abstract': 'This paper considers the problem of bounded rational human behavior in the cooperative human-machine game. The cooperation between human and machine is a raising topic for emergency handling, and it is critical to ensure the safety of human. First, a barrier-function-based state transformation is developed to ensure the safety constraints of the human-machine system state. A level-k rationality structure is then exploited by cognitive hierarchy to learn human behavior, and the bounded rational behavior is obtained by using Adaptive Dynamic Programming ({ADP}). Inspired by behavior modeling from sociology, a softmax probabilistic decision distribution is utilized to model human behavior, which imitates the true impact of human in the cooperative game. Finally, a simulation is implemented to test the effectiveness of the proposed behavior, which demonstrates that the full state constraints and stabilization are guaranteed.',\n",
       "  'doi': '10.1109/ICDL55364.2023.10364413',\n",
       "  'url': 'https://ieeexplore.ieee.org/document/10364413/',\n",
       "  'title': 'Safe Human-Machine Cooperative Game with Level-k Rationality Modeled Human Impact',\n",
       "  'ENTRYTYPE': 'inproceedings',\n",
       "  'ID': 'tanSafeHumanMachineCooperative2023'},\n",
       " {'file': 'IEEE Xplore Abstract Record:/Users/tanjunkai/Zotero/storage/2VS2S24B/10665364.html:text/html;IEEE Xplore Full Text PDF:/Users/tanjunkai/Zotero/storage/DESTG3JG/Tan 等 - 2024 - Safe Stabilization Control for Interconnected Virtual-Real Systems via Model-based Reinforcement Lea.pdf:application/pdf',\n",
       "  'keywords': 'Estimation error, Interconnected virtual-real system, Learning systems, Mathematical models, notion, Numerical models, Numerical simulation, Optimal control, reinforcement learning, Reinforcement learning, safety-guaranteed, stabilization control',\n",
       "  'note': '{EI}: 是',\n",
       "  'langid': 'english',\n",
       "  'date': '2024-07',\n",
       "  'urldate': '2024-09-29',\n",
       "  'author': 'Tan, Jun Kai and Xue, Shuang Si and Li, Huan and Cao, Hui and Li, Dong Yu',\n",
       "  'booktitle': '2024 14th Asian Control Conference (ascc)',\n",
       "  'pages': '605--610',\n",
       "  'eventtitle': '2024 14th Asian Control Conference ({ASCC})',\n",
       "  'abstract': 'In this paper, a safe-guarding controller is designed for the interconnected virtual-real system based on a reinforcement learning framework to achieve stabilization control. We established the mathematical formulation of the interconnected virtual-real system and the safety-guaranteed stabilization optimization problem. Online reinforcement learning methods are utilized to solve the Hamilton-Jacobi-Bellman({HJB}) equation on the established optimal control problem. The safe-guarding term is introduced to achieve safe-guarding control for the real part. Single network is used to approximate the value function. Concurrent Learning methods are introduced to train the network without excitation risks. We prove that the dynamics of the estimation error of the designed critic network are uniform and ultimately bounded. Finally, a numerical simulation example is provided to illustrate the effectiveness of the proposed control method.',\n",
       "  'url': 'https://ieeexplore.ieee.org/document/10665364/?arnumber=10665364',\n",
       "  'title': 'Safe stabilization control for interconnected virtual-real systems via model-based reinforcement learning',\n",
       "  'ENTRYTYPE': 'inproceedings',\n",
       "  'ID': 'tanSafeStabilizationControl2024'},\n",
       " {'file': 'PDF:/Users/tanjunkai/Zotero/storage/S3LCSCIX/Tan 等 - 2025 - Data-driven optimal shared control of unmanned aerial vehicles.pdf:application/pdf',\n",
       "  'keywords': '/unread',\n",
       "  'note': '{GSCC}: 0000002 2025-03-25T02:45:55.713Z \\n{JCR分区}: Q1\\n中科院分区升级版: 计算机科学2区\\n影响因子: 5.5\\n5年影响因子: 5.5\\n{EI}: 是',\n",
       "  'langid': 'english',\n",
       "  'date': '2025-03-14',\n",
       "  'urldate': '2025-02-03',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Guo, Zihang and Li, Huan and Cao, Hui and Chen, Badong',\n",
       "  'shortjournal': 'Neurocomputing',\n",
       "  'journaltitle': 'Neurocomputing',\n",
       "  'pages': '129428',\n",
       "  'abstract': 'Cooperation between humans and autonomy is a critical topic of unmanned aerial vehicle ({UAV}) control. How to co-pilot the {UAV} with human operator to achieve optimal performance presents a significant challenge. In this paper, we propose a novel data-driven optimal shared control method for {UAV} using the Koopman operators to predict the nonlinear dynamics of the {UAVs}. An original shared control mechanism is established to allocate the relationship between optimal and human control inputs. The model of the system is learned from human maneuver data via the Koopman operator approach, and the optimal controller is approximated online using reinforcement learning techniques. The Lyapunov theory analyzes the stability of the proposed method. Compared with offline {RL} methods, the proposed method can learn the optimal controller online without a precise {UAV} dynamics model from human maneuver data. The effectiveness of the proposed method is demonstrated by numerical and Human-in-the-loop ({HiTL}) simulation.',\n",
       "  'doi': '10.1016/j.neucom.2025.129428',\n",
       "  'url': 'https://www.sciencedirect.com/science/article/abs/pii/S0925231225001006',\n",
       "  'issn': '09252312',\n",
       "  'volume': '622',\n",
       "  'title': 'Data-driven optimal shared control of unmanned aerial vehicles',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanDatadrivenOptimalShared2025'},\n",
       " {'file': 'PDF:/Users/tanjunkai/Zotero/storage/B3T3352J/Tan 等 - 2025 - Human–AI interactive optimized shared control.pdf:application/pdf',\n",
       "  'keywords': '/unread',\n",
       "  'note': '{GSCC}: 0000003 2025-03-25T02:45:50.906Z \\n0 citations (Semantic Scholar/{DOI}) [2025-03-11]\\n0 citations (Semantic Scholar/{DOI}) [2025-03-11]\\n0 citations (Crossref/{DOI}) [2025-03-11]',\n",
       "  'langid': 'english',\n",
       "  'date': '2025-01-07',\n",
       "  'urldate': '2025-02-07',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Cao, Hui and Ge, Shuzhi Sam',\n",
       "  'shortjournal': 'Journal of Automation and Intelligence',\n",
       "  'journaltitle': 'Journal of Automation and Intelligence',\n",
       "  'pages': 'S2949855425000024',\n",
       "  'abstract': 'This paper presents an optimized shared control algorithm for human–{AI} interaction, implemented through a digital twin framework where the physical system and human operator act as the real agent while an {AIdriven} digital system functions as the virtual agent. In this digital twin architecture, the real agent acquires an optimal control strategy through observed actions, while the {AI} virtual agent mirrors the real agent to establish a digital replica system and corresponding control policy. Both the real and virtual optimal controllers are approximated using reinforcement learning ({RL}) techniques. Specifically, critic neural networks ({NNs}) are employed to learn the virtual and real optimal value functions, while actor {NNs} are trained to derive their respective optimal controllers. A novel shared mechanism is introduced to integrate both virtual and real value functions into a unified learning framework, yielding an optimal shared controller. This controller adaptively adjusts the confidence ratio between virtual and real agents, enhancing the system’s efficiency and flexibility in handling complex control tasks. The stability of the closed-loop system is rigorously analyzed using the Lyapunov method. The effectiveness of the proposed {AI}–human interactive system is validated through two numerical examples: a representative nonlinear system and an unmanned aerial vehicle ({UAV}) control system.',\n",
       "  'doi': '10.1016/j.jai.2025.01.001',\n",
       "  'url': 'https://linkinghub.elsevier.com/retrieve/pii/S2949855425000024',\n",
       "  'issn': '29498554',\n",
       "  'title': 'Human–{AI} interactive optimized shared control',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanHumanAIInteractive2025'},\n",
       " {'file': 'PDF:/Users/tanjunkai/Zotero/storage/VEQE7AZT/Tan 等 - Stackelberg Game-based Robust Optimal Control of Cyber-Physical System Under Hybrid Attacks.pdf:application/pdf',\n",
       "  'langid': 'english',\n",
       "  'date': '2025-02-27',\n",
       "  'author': 'Tan, Jun Kai and Xue, Shuang Si and Cao, Hui',\n",
       "  'shortjournal': 'Int. J. Intell. Control Syst.',\n",
       "  'journaltitle': 'The International Journal of Intelligent Control and Systems',\n",
       "  'pages': '--9',\n",
       "  'abstract': 'This paper presents a novel framework integrating Stackelberg game theory and reinforcement learning for cyberphysical system ({CPS}) security. We develop a hierarchical game model where defenders and attackers interact through sequential decision-making. The defender-attacker dynamics are formulated as an optimization problem combining H2 and H∞ control objectives. Key innovations include: 1) A unified game-theoretic approach for modeling hybrid attack-defense mechanisms, 2) Online reinforcement learning algorithms for real-time strategy adaptation, and 3) Rigorous stability analysis using Lyapunov theory. Theoretical guarantees of convergence are established for the proposed learning scheme. Comprehensive experiments on a robotic platform validate the framework’s effectiveness in maintaining control performance under diverse attack scenarios.',\n",
       "  'title': 'Stackelberg game-based robust optimal control of cyber-physical system under hybrid attacks',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanStackelbergGamebasedRobust2025'},\n",
       " {'file': 'Full Text PDF:/Users/tanjunkai/Zotero/storage/7KYKA97X/Tan 等 - Human–Machine Shared Stabilization Control Based on Safe Adaptive Dynamic Programming With Bounded R.pdf:application/pdf;Snapshot:/Users/tanjunkai/Zotero/storage/CZECWYSF/rnc.html:text/html',\n",
       "  'keywords': 'adaptive dynamic programming, barrier function, bounded rationality, human–machine collaboration, shared control',\n",
       "  'note': '{JCR分区}: Q1\\n中科院分区升级版: 计算机科学4区\\n影响因子: 3.2\\n5年影响因子: 3.5\\n{EI}: 是',\n",
       "  'langid': 'english',\n",
       "  'date': '2025-03-24',\n",
       "  'urldate': '2025-03-24',\n",
       "  'author': 'Tan, Junkai and Wang, Jingcheng and Xue, Shuangsi and Cao, Hui and Li, Huan and Guo, Zihang',\n",
       "  'journaltitle': 'International Journal of Robust and Nonlinear Control',\n",
       "  'issue': 'n/a',\n",
       "  'abstract': 'This article considers the shared control of bounded rational human behavior with cooperative autonomous machines. For the collaboration of humans and machines, it is crucial to ensure the safety of the interactive process due to the involvement of human beings. First, a barrier-function-based state transformation is developed to ensure full state safety constraints. A level-k{\\\\textbackslash} k {\\\\textbackslash} thinking framework is exploited to obtain bounded rationality. Every single level-k{\\\\textbackslash} k {\\\\textbackslash} control policy is approximated by using adaptive dynamic programming. Inspired by the theory of human behavior modeling, a probabilistic distribution based on Softmax is utilized to model human behavior, which imitates the uncertainty of human intelligence in the cooperative game. Through the construction of a shared control framework, the control inputs of humans and machines are blended to achieve stabilization safely and efficiently. Finally, simulations are implemented to test the effectiveness of the proposed cooperation architecture. The result demonstrates that full-state asymmetric constraints and stabilization are guaranteed in commonly safety-critical situations, and the shared control framework ensures the safety of the overall system when one of the participants is not safety-aware.',\n",
       "  'doi': '10.1002/rnc.7931',\n",
       "  'url': 'https://onlinelibrary.wiley.com/doi/abs/10.1002/rnc.7931',\n",
       "  'issn': '1099-1239',\n",
       "  'rights': '© 2025 John Wiley \\\\& Sons Ltd.',\n",
       "  'volume': 'n/a',\n",
       "  'title': 'Human–Machine Shared Stabilization Control Based on Safe Adaptive Dynamic Programming With Bounded Rationality',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanHumanMachineShared2025'},\n",
       " {'file': 'PDF:/Users/tanjunkai/Zotero/storage/G6B7QRJ7/Tan 等 - 2025 - Finite-time Safe Reinforcement Learning Control of Multi-player Nonzero-Sum Game for Quadcopter Syst.pdf:application/pdf;ScienceDirect Snapshot:/Users/tanjunkai/Zotero/storage/HNACTTAX/S002002552500249X.html:text/html',\n",
       "  'keywords': 'adaptive dynamic programming, dynamic event-trigger, Finite-time optimal control, neural network, nonzero-sum game, reinforcement learning',\n",
       "  'note': 'https://github.com/tanjunkai2001/{FT}-{SRL}-Quadcopter\\n{EI}: 是',\n",
       "  'date': '2025-03-24',\n",
       "  'urldate': '2025-03-25',\n",
       "  'author': 'Tan, Junkai and Xue, Shuangsi and Guan, Qingshu and Qu, Kai and Cao, Hui',\n",
       "  'shortjournal': 'Information Sciences',\n",
       "  'journaltitle': 'Information Sciences',\n",
       "  'pages': '122117',\n",
       "  'abstract': 'This paper investigates a finite-time safe reinforcement learning control algorithm for multi-player nonzero-sum games ({FT}-{SRL}-{NZS}). In addressing the finite-time safe optimal control issue, value functions incorporating designated barrier functions for the involved players are established within the transformed finite-time stable space. The finite-time safe optimal controller is derived from the solution to the transformed Nash equilibrium condition. An actor-critic structure is proposed for solving the Hamilton-Jacobi-Bellman ({HJB}) equation in the finite-time stable space, aimed at approximating the finite-time optimal value and its corresponded controller using a novel finite-time concurrent learning update law. A dynamic event-trigger rule adjusts the trigger condition in real time, thereby minimizing the computational and communicative demands associated with calculating Nash equilibrium. Lyapunov stability analysis is employed to examine the finite-time equilibrium of the closed-loop system. Numerical simulations and unmanned aerial vehicle ({UAV}) hardware tests are carried out to illustrate the efficacy of the proposed finite-time safe control algorithm.',\n",
       "  'doi': '10.1016/j.ins.2025.122117',\n",
       "  'url': 'https://www.sciencedirect.com/science/article/pii/S002002552500249X',\n",
       "  'issn': '0020-0255',\n",
       "  'title': 'Finite-time Safe Reinforcement Learning Control of Multi-player Nonzero-Sum Game for Quadcopter Systems',\n",
       "  'ENTRYTYPE': 'article',\n",
       "  'ID': 'tanFinitetimeSafeReinforcement2025'}]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = bibtexparser.bparser.BibTexParser(common_strings=True, ignore_nonstandard_types=False)\n",
    "with open(\"bibliography.bib\") as bibtex_file:\n",
    "    publications = bibtexparser.load(bibtex_file, parser=parser)\n",
    "\n",
    "publications.entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    \"\"\"Produce entities within text.\"\"\"\n",
    "    return \"\".join(html_escape_table.get(c,c) for c in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_author(a):\n",
    "    \"\"\"Parses author name in the format of `Last, First Middle` where\n",
    "    the middle name is sometimes there and sometimes not (and could just be an initial)\n",
    "    \n",
    "    Returns: author name as `F. M. Last`\n",
    "    \"\"\"\n",
    "    \n",
    "    a = a.split(', ')\n",
    "    last = a[0].strip()\n",
    "    fm = a[1].split(' ')\n",
    "    first = fm[0][0] + '.'\n",
    "    \n",
    "    if len(fm) > 1:\n",
    "        middle = fm[1][0] + '.'\n",
    "    else:\n",
    "        middle = ''\n",
    "#     print([first, middle, last])\n",
    "    \n",
    "    if not middle == '':\n",
    "        return first + ' ' + middle + ' ' + last\n",
    "    else:\n",
    "        return first + ' ' + last\n",
    "\n",
    "# for row, item in publications.iterrows():\n",
    "#     authors = ', '.join([parse_author(a) for a in item['Author'].split('; ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'PDF:/Users/tanjunkai/Zotero/storage/G6B7QRJ7/Tan 等 - 2025 - Finite-time Safe Reinforcement Learning Control of Multi-player Nonzero-Sum Game for Quadcopter Syst.pdf:application/pdf;ScienceDirect Snapshot:/Users/tanjunkai/Zotero/storage/HNACTTAX/S002002552500249X.html:text/html',\n",
       " 'keywords': 'adaptive dynamic programming, dynamic event-trigger, Finite-time optimal control, neural network, nonzero-sum game, reinforcement learning',\n",
       " 'note': 'https://github.com/tanjunkai2001/{FT}-{SRL}-Quadcopter\\n{EI}: 是',\n",
       " 'date': '2025-03-24',\n",
       " 'urldate': '2025-03-25',\n",
       " 'author': 'Tan, Junkai and Xue, Shuangsi and Guan, Qingshu and Qu, Kai and Cao, Hui',\n",
       " 'shortjournal': 'Information Sciences',\n",
       " 'journaltitle': 'Information Sciences',\n",
       " 'pages': '122117',\n",
       " 'abstract': 'This paper investigates a finite-time safe reinforcement learning control algorithm for multi-player nonzero-sum games ({FT}-{SRL}-{NZS}). In addressing the finite-time safe optimal control issue, value functions incorporating designated barrier functions for the involved players are established within the transformed finite-time stable space. The finite-time safe optimal controller is derived from the solution to the transformed Nash equilibrium condition. An actor-critic structure is proposed for solving the Hamilton-Jacobi-Bellman ({HJB}) equation in the finite-time stable space, aimed at approximating the finite-time optimal value and its corresponded controller using a novel finite-time concurrent learning update law. A dynamic event-trigger rule adjusts the trigger condition in real time, thereby minimizing the computational and communicative demands associated with calculating Nash equilibrium. Lyapunov stability analysis is employed to examine the finite-time equilibrium of the closed-loop system. Numerical simulations and unmanned aerial vehicle ({UAV}) hardware tests are carried out to illustrate the efficacy of the proposed finite-time safe control algorithm.',\n",
       " 'doi': '10.1016/j.ins.2025.122117',\n",
       " 'url': 'https://www.sciencedirect.com/science/article/pii/S002002552500249X',\n",
       " 'issn': '0020-0255',\n",
       " 'title': 'Finite-time Safe Reinforcement Learning Control of Multi-player Nonzero-Sum Game for Quadcopter Systems',\n",
       " 'ENTRYTYPE': 'article',\n",
       " 'ID': 'tanFinitetimeSafeReinforcement2025',\n",
       " 'plain_file': 'PDF:/Users/tanjunkai/Zotero/storage/G6B7QRJ7/Tan 等 - 2025 - Finite-time Safe Reinforcement Learning Control of Multi-player Nonzero-Sum Game for Quadcopter Syst.pdf:application/pdf;ScienceDirect Snapshot:/Users/tanjunkai/Zotero/storage/HNACTTAX/S002002552500249X.html:text/html',\n",
       " 'plain_keywords': 'adaptive dynamic programming, dynamic event-trigger, Finite-time optimal control, neural network, nonzero-sum game, reinforcement learning',\n",
       " 'plain_note': 'https://github.com/tanjunkai2001/FT-SRL-Quadcopter\\nEI: 是',\n",
       " 'plain_date': '2025-03-24',\n",
       " 'plain_urldate': '2025-03-25',\n",
       " 'plain_author': 'Tan, Junkai and Xue, Shuangsi and Guan, Qingshu and Qu, Kai and Cao, Hui',\n",
       " 'plain_shortjournal': 'Information Sciences',\n",
       " 'plain_journaltitle': 'Information Sciences',\n",
       " 'plain_pages': '122117',\n",
       " 'plain_abstract': 'This paper investigates a finite-time safe reinforcement learning control algorithm for multi-player nonzero-sum games (FT-SRL-NZS). In addressing the finite-time safe optimal control issue, value functions incorporating designated barrier functions for the involved players are established within the transformed finite-time stable space. The finite-time safe optimal controller is derived from the solution to the transformed Nash equilibrium condition. An actor-critic structure is proposed for solving the Hamilton-Jacobi-Bellman (HJB) equation in the finite-time stable space, aimed at approximating the finite-time optimal value and its corresponded controller using a novel finite-time concurrent learning update law. A dynamic event-trigger rule adjusts the trigger condition in real time, thereby minimizing the computational and communicative demands associated with calculating Nash equilibrium. Lyapunov stability analysis is employed to examine the finite-time equilibrium of the closed-loop system. Numerical simulations and unmanned aerial vehicle (UAV) hardware tests are carried out to illustrate the efficacy of the proposed finite-time safe control algorithm.',\n",
       " 'plain_doi': '10.1016/j.ins.2025.122117',\n",
       " 'plain_url': 'https://www.sciencedirect.com/science/article/pii/S002002552500249X',\n",
       " 'plain_issn': '0020-0255',\n",
       " 'plain_title': 'Finite-time Safe Reinforcement Learning Control of Multi-player Nonzero-Sum Game for Quadcopter Systems',\n",
       " 'plain_ENTRYTYPE': 'article',\n",
       " 'plain_ID': 'tanFinitetimeSafeReinforcement2025'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = publications.entries[-1]\n",
    "p = bibtexparser.customization.add_plaintext_fields(p)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "for item in publications.entries:\n",
    "    \n",
    "    item = bibtexparser.customization.add_plaintext_fields(item)\n",
    "    year = item['plain_year'] if 'plain_year' in item else item['plain_date'].split('-')[0]\n",
    "    key = item['plain_author'].split(',')[0].replace(' ', '').lower() + str(year) + item['plain_title'].split(' ')[0].lower()\n",
    "\n",
    "    md_filename = key + \".md\"\n",
    "    html_filename = key\n",
    "    \n",
    "    ## YAML variables\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item['plain_title'] + '\"\\n'\n",
    "    \n",
    "    md += \"\"\"collection: publications\"\"\"\n",
    "    \n",
    "    md += \"\"\"\\npermalink: /publication/\"\"\" + html_filename\n",
    "    \n",
    "#     month = month_dict[item['plain_month']] if 'plain_month' in item else 6\n",
    "#     day = item['plain_day'] if 'plain_day' in item else 15\n",
    "#     date = datetime.date(int(year), int(month), int(day)).isoformat() + \"00:00:00 + 0500\"\n",
    "    if not 'plain_date' in item:\n",
    "        raise Exception(item)\n",
    "    date = item['plain_date']\n",
    "    if len(date) == 4:\n",
    "        date += \"-06-15 00:00:00 +0500\"\n",
    "    elif len(date) == 7:\n",
    "        date += \"-15 00:00:00 +0500\"\n",
    "    elif len(date) == 10:\n",
    "        date += \" 00:00:00 +0500\"\n",
    "    else:\n",
    "        print(date)\n",
    "        break\n",
    "    \n",
    "    \n",
    "    md += \"\\ndate: \" + date\n",
    "\n",
    "    if 'plain_eventtitle' in item:\n",
    "        venue = item['plain_eventtitle']\n",
    "    elif 'plain_booktitle' in item:\n",
    "        venue = item['plain_booktitle']\n",
    "    elif 'plain_journaltitle' in item:\n",
    "        venue = item['plain_journaltitle']\n",
    "    elif 'plain_school' in item:\n",
    "        venue = item['plain_institution']\n",
    "    else:\n",
    "        venue = False\n",
    "    \n",
    "    if 'plain_note' in item:\n",
    "        note = item['plain_note']\n",
    "        if 'submitted' in note.lower() or 'review' in note.lower() or 'accepted' in note.lower():\n",
    "            venue += \" (<b><i>\" + note + \"</i></b>)\"\n",
    "        \n",
    "        \n",
    "    if venue:\n",
    "        md += \"\\nvenue: '\" + html_escape(venue) + \"'\"\n",
    "    \n",
    "    if 'plain_url' in item:\n",
    "        md += \"\\npaperurl: '\" + item['plain_url'] + \"'\"\n",
    "    \n",
    "    if 'plain_doi' in item:\n",
    "        md += \"\\ndoi: '\" + item['plain_doi'] + \"'\"\n",
    "        \n",
    "    pubtypes = {\"inproceedings\": \"conference\",\n",
    "                \"article\": \"journal\",\n",
    "                \"thesis\": \"academic\"}\n",
    "    md += \"\\npubtype: '\" + pubtypes[item['ENTRYTYPE']] + \"'\"\n",
    "\n",
    "    if item['ENTRYTYPE'] == 'article':\n",
    "        # md += \"\\nimage: '\" + venue + \".jpg'\"\n",
    "        md += \"\\nimage: '../images/\" + venue + \".jpg'\"\n",
    "\n",
    "    if 'github' in note.lower():\n",
    "        code = note.split('github.com/')[-1].split()[0]\n",
    "        md += \"\\ncode: 'https://github.com/\" + code + \"'\"\n",
    "\n",
    "    citation = f\"{item['plain_author']} ({year}). {item['plain_title']}. {venue}.\"\n",
    "    md += \"\\ncitation: '\" + html_escape(citation) + \"'\"\n",
    "    authors = ', '.join([parse_author(a) for a in item['plain_author'].split(' and ')])\n",
    "    md += \"\\nauthors: '\" + authors + \"'\"\n",
    "    \n",
    "    md += \"\\nexcerpt_separator: \\\"\\\"\"\n",
    "    \n",
    "    md += \"\\n---\"\n",
    "    \n",
    "    ## Markdown description for individual page\n",
    "        \n",
    "#     if len(str(item.excerpt)) > 5:\n",
    "#         md += \"\\n\" + html_escape(item.excerpt) + \"\\n\"\n",
    "    \n",
    "    if 'plain_abstract' in item:\n",
    "        md += \"\\n\" + html_escape(item['plain_abstract']) + \"\\n\"\n",
    "    \n",
    "    if 'plain_url' in item:\n",
    "        md += \"\\n[Download paper here](\" + item['plain_url'] + \")\"\n",
    "    \n",
    "    if 'plain_doi' in item:\n",
    "        md += \"\\n[DOI](\" + item['plain_doi'] + \")\"\n",
    "        \n",
    "#     md += \"\\nRecommended citation: \" + item.citation\n",
    "    \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "       \n",
    "    with open(\"../_publications/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the publications directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IEEE Transactions on Automation Science and Engineering.jpg\n",
      "Information Sciences.jpg\n",
      "International Journal of Robust and Nonlinear Control.jpg\n",
      "Journal of Automation and Intelligence.jpg\n",
      "Neurocomputing.jpg\n",
      "Nonlinear Dynamics.jpg\n",
      "tan2023nash.md\n",
      "tan2023safe.md\n",
      "tan2024safe.md\n",
      "tan2025data-driven.md\n",
      "tan2025finite-time.md\n",
      "tan2025human–ai.md\n",
      "tan2025human–machine.md\n",
      "tan2025prescribed.md\n",
      "tan2025stackelberg.md\n",
      "tan2025unmanned.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_publications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ../_publications/behbahani2014aircraft.md: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat ../_publications/behbahani2014aircraft.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
